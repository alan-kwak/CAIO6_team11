{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "\n",
    "import tqdm\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>S&amp;P</th>\n",
       "      <th>currency</th>\n",
       "      <th>gold</th>\n",
       "      <th>kospi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>1070.119995</td>\n",
       "      <td>1198.400024</td>\n",
       "      <td>402.700012</td>\n",
       "      <td>807.390015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-12-02</td>\n",
       "      <td>1066.619995</td>\n",
       "      <td>1195.000000</td>\n",
       "      <td>403.700012</td>\n",
       "      <td>807.780029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-12-03</td>\n",
       "      <td>1064.729980</td>\n",
       "      <td>1192.500000</td>\n",
       "      <td>403.899994</td>\n",
       "      <td>808.340027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-12-04</td>\n",
       "      <td>1069.719971</td>\n",
       "      <td>1190.099976</td>\n",
       "      <td>403.299988</td>\n",
       "      <td>805.130005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-12-05</td>\n",
       "      <td>1061.500000</td>\n",
       "      <td>1184.699951</td>\n",
       "      <td>406.399994</td>\n",
       "      <td>789.409973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date          S&P     currency        gold       kospi\n",
       "0  2003-12-01  1070.119995  1198.400024  402.700012  807.390015\n",
       "1  2003-12-02  1066.619995  1195.000000  403.700012  807.780029\n",
       "2  2003-12-03  1064.729980  1192.500000  403.899994  808.340027\n",
       "3  2003-12-04  1069.719971  1190.099976  403.299988  805.130005\n",
       "4  2003-12-05  1061.500000  1184.699951  406.399994  789.409973"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./total.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5320 entries, 0 to 5319\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Date      5320 non-null   object \n",
      " 1   S&P       5320 non-null   float64\n",
      " 2   currency  5320 non-null   float64\n",
      " 3   gold      5320 non-null   float64\n",
      " 4   kospi     5320 non-null   float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 207.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'S&P', 'currency', 'gold', 'kospi'], dtype='object')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_1 = df.sort_index(ascending=False).copy()\n",
    "# df_1.head()\n",
    "col_names = df.columns\n",
    "col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIM = 128\n",
    "DAY_INT = 2\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4256, 5), (1064, 5))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian transformation\n",
    "\n",
    "mean_var = {}\n",
    "mean_var['S&P'] = [df['S&P'].mean(), df['S&P'].var()]\n",
    "mean_var['currency'] = [df['currency'].mean(), df['currency'].var()]\n",
    "mean_var['gold'] = [df['gold'].mean(), df['gold'].var()]\n",
    "mean_var['kospi'] = [df['kospi'].mean(), df['kospi'].var()]\n",
    "data_names = ['S&P', 'currency', 'gold', 'kospi']\n",
    "\n",
    "for col_name in data_names:\n",
    "    df[col_name] = df[col_name].apply(lambda x: (x - mean_var[col_name][0])/mean_var[col_name][1])\n",
    "\n",
    "# train-test split\n",
    "train_data = df.iloc[:int(len(df) * 0.8), :]\n",
    "test_data = df.iloc[int(len(df) * 0.8):, :]\n",
    "\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stockdataset(Dataset):\n",
    "    def __init__(self, data, input_size=128, day_interval=2):\n",
    "        self.data = data\n",
    "        self.len = len(data)\n",
    "        start_pos = (self.len - input_size -1) % day_interval\n",
    "        print(self.len, start_pos, (self.len - input_size -1) // day_interval)\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i in range((self.len - input_size -1) // day_interval + 1):  # check iteration number \n",
    "            start = start_pos + i * day_interval\n",
    "            end = start + input_size - 1    # -1 due to slicing loc not iloc\n",
    "            x_num = df.loc[start:end, ['S&P', 'currency', 'gold', 'kospi']].to_numpy()\n",
    "            X.append(x_num.T)\n",
    "            Y.append(df.loc[end + 1, 'kospi'])\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "        self.len = len(X)\n",
    "        print(start, end)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4256 1 2063\n",
      "4127 4254\n",
      "1064 1 467\n",
      "935 1062\n",
      "(array([[-0.00086512, -0.00086659, -0.00086271, -0.00086911, -0.00086303,\n",
      "        -0.00087014, -0.00087102, -0.00086155, -0.00085927, -0.00086402,\n",
      "        -0.00085849, -0.00085744, -0.00084755, -0.00084796, -0.00084463,\n",
      "        -0.00084223, -0.00084377, -0.00084377, -0.00084233, -0.00083175,\n",
      "        -0.00083162, -0.00082985, -0.00082985, -0.00083253, -0.00082183,\n",
      "        -0.0008207 , -0.00081863, -0.00081427, -0.00082211, -0.00081793,\n",
      "        -0.00082261, -0.00081536, -0.00081417, -0.00080811, -0.00080811,\n",
      "        -0.00080894, -0.00080205, -0.00080491, -0.00080677, -0.00079601,\n",
      "        -0.00080483, -0.00081695, -0.00081257, -0.00081489, -0.00081167,\n",
      "        -0.00081107, -0.00081848, -0.00081687, -0.00080583, -0.00080813,\n",
      "        -0.00080367, -0.00079415, -0.00079855, -0.00080346, -0.00080346,\n",
      "        -0.00079475, -0.00079878, -0.00080248, -0.00080478, -0.00080721,\n",
      "        -0.00080869, -0.00080512, -0.00080416, -0.00080414, -0.00079555,\n",
      "        -0.0008009 , -0.00079939, -0.0007964 , -0.00079485, -0.00080238,\n",
      "        -0.00080753, -0.00082053, -0.00083385, -0.00082311, -0.00083563,\n",
      "        -0.0008308 , -0.00082064, -0.00082175, -0.00083151, -0.00084271,\n",
      "        -0.00084384, -0.00084588, -0.00083197, -0.00083285, -0.00082163,\n",
      "        -0.0008181 , -0.00081872, -0.00081408, -0.00080657, -0.00079975,\n",
      "        -0.00080163, -0.00080757, -0.00080851, -0.00080851, -0.00080393,\n",
      "        -0.0008162 , -0.00081719, -0.00081667, -0.00081218, -0.00081124,\n",
      "        -0.000825  , -0.00082037, -0.00080804, -0.00080751, -0.00081146,\n",
      "        -0.00080945, -0.00082168, -0.00082831, -0.00083344, -0.00082551,\n",
      "        -0.00082391, -0.00082236, -0.00082824, -0.00084014, -0.00084916,\n",
      "        -0.00084267, -0.00084125, -0.0008419 , -0.00084248, -0.00085151,\n",
      "        -0.00084576, -0.00084794, -0.00084755, -0.00084414, -0.0008427 ,\n",
      "        -0.00082897, -0.0008275 , -0.00082256],\n",
      "       [ 0.00525065,  0.0050498 ,  0.00485698,  0.00442314,  0.00449544,\n",
      "         0.00426245,  0.00375631,  0.0032341 ,  0.00349922,  0.00403751,\n",
      "         0.00402947,  0.00460792,  0.00435084,  0.0050739 ,  0.00529082,\n",
      "         0.00528279,  0.00569252,  0.00559612,  0.00570859,  0.00453561,\n",
      "         0.00523458,  0.00484091,  0.00417408,  0.00531493,  0.00395716,\n",
      "         0.00480074,  0.00378845,  0.00332247,  0.00414998,  0.00314573,\n",
      "         0.00366794,  0.0036599 ,  0.00360366,  0.00458382,  0.00464809,\n",
      "         0.00378041,  0.00438296,  0.00466416,  0.0036599 ,  0.00360366,\n",
      "         0.00306538,  0.00375631,  0.00229411,  0.00243872,  0.00231018,\n",
      "         0.00326624,  0.00315376,  0.00225393,  0.00302521,  0.00186027,\n",
      "         0.00288863,  0.00251103,  0.00237445,  0.00241462,  0.00145053,\n",
      "         0.00218967,  0.00145053,  0.00139429,  0.00223787,  0.00399734,\n",
      "         0.00405357,  0.00234232,  0.0029047 ,  0.00346709,  0.00379648,\n",
      "         0.00267974,  0.00216556,  0.00243872,  0.00370007,  0.00216556,\n",
      "         0.00415802,  0.00235035,  0.0025994 ,  0.00380452,  0.00327427,\n",
      "         0.00335461,  0.00360366,  0.00238249,  0.0032823 ,  0.00268778,\n",
      "         0.00270385,  0.0042705 ,  0.00432673,  0.00439101,  0.0040616 ,\n",
      "         0.00394109,  0.00294487,  0.00255923,  0.00437493,  0.00528279,\n",
      "         0.00435084,  0.00018115,  0.00039003,  0.00032576,  0.0004302 ,\n",
      "         0.00100865,  0.00230215,  0.00181206,  0.00251906,  0.00210933,\n",
      "         0.00240658,  0.00232624,  0.00238249,  0.00217359,  0.00095242,\n",
      "         0.00183617,  0.00119344,  0.00320999,  0.00265564,  0.00244675,\n",
      "         0.00343495,  0.00306538,  0.0029047 ,  0.00242266,  0.00392503,\n",
      "         0.00447135,  0.00312162,  0.00458382,  0.00455972,  0.00405357,\n",
      "         0.00434279,  0.00353135,  0.00356349,  0.00345905,  0.00399734,\n",
      "         0.00403751,  0.0039893 ,  0.00315376],\n",
      "       [-0.00389919, -0.00389826, -0.00390103, -0.00388673, -0.0038858 ,\n",
      "        -0.00387842, -0.00388765, -0.00389503, -0.00387288, -0.0038738 ,\n",
      "        -0.00388073, -0.00386088, -0.00386827, -0.0038738 , -0.00386734,\n",
      "        -0.0038655 , -0.0038595 , -0.0038595 , -0.0038595 , -0.00384796,\n",
      "        -0.00383827, -0.00384381, -0.00384381, -0.00384381, -0.00380366,\n",
      "        -0.00381104, -0.0038152 , -0.0038055 , -0.00379443, -0.00379489,\n",
      "        -0.00380689, -0.00381612, -0.0038775 , -0.00388534, -0.00388534,\n",
      "        -0.00385765, -0.00386504, -0.00387011, -0.0038798 , -0.0038858 ,\n",
      "        -0.00387011, -0.00384935, -0.00392318, -0.00390611, -0.00392226,\n",
      "        -0.00391995, -0.00391165, -0.00392503, -0.00389965, -0.00388488,\n",
      "        -0.00388673, -0.00386919, -0.00385304, -0.00386873, -0.00386873,\n",
      "        -0.00384242, -0.0038595 , -0.00387104, -0.0039278 , -0.00392088,\n",
      "        -0.00389549, -0.00393564, -0.00393564, -0.00393287, -0.00391949,\n",
      "        -0.00394626, -0.00395133, -0.00394903, -0.00391026, -0.00391349,\n",
      "        -0.00389688, -0.00391626, -0.00391303, -0.00393795, -0.00391903,\n",
      "        -0.00390519, -0.00388442, -0.00386504, -0.00385857, -0.00383596,\n",
      "        -0.00382489, -0.00383688, -0.00383919, -0.00381427, -0.00383735,\n",
      "        -0.00381612, -0.00379027, -0.00378797, -0.00381658, -0.00384519,\n",
      "        -0.00382904, -0.00381104, -0.00382442, -0.00382442, -0.0038235 ,\n",
      "        -0.00388396, -0.00391718, -0.00392687, -0.00391165, -0.00391349,\n",
      "        -0.00392687, -0.00395872, -0.00394718, -0.00393887, -0.00393472,\n",
      "        -0.00392272, -0.00398364, -0.0039781 , -0.00397625, -0.00397625,\n",
      "        -0.00395595, -0.00394672, -0.00397164, -0.00401456, -0.0040164 ,\n",
      "        -0.00402286, -0.00402056, -0.00403256, -0.0040224 , -0.00401087,\n",
      "        -0.00402794, -0.00399518, -0.00401594, -0.00398641, -0.00398271,\n",
      "        -0.00397025, -0.00397025, -0.0039398 ],\n",
      "       [-0.00388166, -0.00387974, -0.00389076, -0.00394474, -0.00396057,\n",
      "        -0.00395182, -0.00392678, -0.00393884, -0.0038875 , -0.00383228,\n",
      "        -0.00387132, -0.00390807, -0.00388262, -0.00386992, -0.00389279,\n",
      "        -0.00390192, -0.00393396, -0.00393396, -0.00394667, -0.00393434,\n",
      "        -0.0038716 , -0.0038716 , -0.0038716 , -0.00383537, -0.00382562,\n",
      "        -0.00382792, -0.00381542, -0.00382544, -0.00375292, -0.00373396,\n",
      "        -0.00374207, -0.00373798, -0.00375158, -0.00374372, -0.00371332,\n",
      "        -0.00369763, -0.00369763, -0.00369763, -0.00369763, -0.00367129,\n",
      "        -0.00369193, -0.00370374, -0.00372476, -0.00374183, -0.00371988,\n",
      "        -0.00377146, -0.00378647, -0.00376786, -0.00373589, -0.00368596,\n",
      "        -0.00367898, -0.00364622, -0.0036407 , -0.00362617, -0.00362926,\n",
      "        -0.00361717, -0.00364361, -0.00362799, -0.00364227, -0.00364217,\n",
      "        -0.00368657, -0.00367874, -0.00368565, -0.00362191, -0.00362191,\n",
      "        -0.00356769, -0.00357936, -0.00353946, -0.0035465 , -0.00356463,\n",
      "        -0.00359389, -0.00364732, -0.00366824, -0.0037408 , -0.00372891,\n",
      "        -0.00373623, -0.00365982, -0.00365831, -0.00362222, -0.00368966,\n",
      "        -0.00368115, -0.00369643, -0.00372507, -0.00368877, -0.00365196,\n",
      "        -0.00365611, -0.00363194, -0.00362421, -0.00362098, -0.00362098,\n",
      "        -0.00354372, -0.00353088, -0.00350708, -0.00354629, -0.00350021,\n",
      "        -0.00350443, -0.00350897, -0.00350897, -0.00356882, -0.00355776,\n",
      "        -0.00350007, -0.00346213, -0.00348252, -0.00344114, -0.00349719,\n",
      "        -0.00351185, -0.00355869, -0.00364942, -0.00369258, -0.00368135,\n",
      "        -0.00367665, -0.00367665, -0.00377898, -0.00377534, -0.00394038,\n",
      "        -0.00393921, -0.00384969, -0.00394227, -0.00401668, -0.00415226,\n",
      "        -0.00410758, -0.0039841 , -0.00401899, -0.00395522, -0.00390961,\n",
      "        -0.00396311, -0.00396311, -0.00389993]]), -0.0038516807413470863)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Stockdataset(train_data, input_size=IN_DIM, day_interval=DAY_INT)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = Stockdataset(test_data, input_size=IN_DIM, day_interval=DAY_INT)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(next(iter(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>S&amp;P</th>\n",
       "      <th>currency</th>\n",
       "      <th>gold</th>\n",
       "      <th>kospi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5310</th>\n",
       "      <td>2024-04-08</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>0.017725</td>\n",
       "      <td>0.004998</td>\n",
       "      <td>0.002677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5311</th>\n",
       "      <td>2024-04-09</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.018043</td>\n",
       "      <td>0.005053</td>\n",
       "      <td>0.002634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5312</th>\n",
       "      <td>2024-04-10</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>0.017586</td>\n",
       "      <td>0.004989</td>\n",
       "      <td>0.002634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5313</th>\n",
       "      <td>2024-04-11</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.018809</td>\n",
       "      <td>0.005105</td>\n",
       "      <td>0.002640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5314</th>\n",
       "      <td>2024-04-12</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>0.019137</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>0.002554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>2024-04-15</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>0.020082</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>0.002515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>2024-04-16</td>\n",
       "      <td>0.002238</td>\n",
       "      <td>0.020730</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>0.002306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.020703</td>\n",
       "      <td>0.005183</td>\n",
       "      <td>0.002219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>0.020077</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>0.002392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>2024-04-19</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>0.019666</td>\n",
       "      <td>0.005344</td>\n",
       "      <td>0.002245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       S&P  currency      gold     kospi\n",
       "5310  2024-04-08  0.002355  0.017725  0.004998  0.002677\n",
       "5311  2024-04-09  0.002361  0.018043  0.005053  0.002634\n",
       "5312  2024-04-10  0.002323  0.017586  0.004989  0.002634\n",
       "5313  2024-04-11  0.002353  0.018809  0.005105  0.002640\n",
       "5314  2024-04-12  0.002294  0.019137  0.005111  0.002554\n",
       "5315  2024-04-15  0.002246  0.020082  0.005156  0.002515\n",
       "5316  2024-04-16  0.002238  0.020730  0.005271  0.002306\n",
       "5317  2024-04-17  0.002215  0.020703  0.005183  0.002219\n",
       "5318  2024-04-18  0.002206  0.020077  0.005232  0.002392\n",
       "5319  2024-04-19  0.002172  0.019666  0.005344  0.002245"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Make transformer encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06) Transformer - disable decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # Transformer의 Positional Encoding 정의\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # 입력 값의 최대 길이만큼 0인 텐서 값 생성\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # sin 주파수 기능 사용\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # cos 주파수 기능 사용\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]  # 각 입력값마다 positional encoding 진행\n",
    "\n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    # Transformer Encoder 구조 정의\n",
    "    def __init__(self,feature_size=64, num_layers=6, dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        \n",
    "        # torch.nn 모듈에 있는 encoder 및 decoder 레이어 설정\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=8, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=feature_size, nhead=8, dropout=dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.decoder = nn.Linear(feature_size, 1)\n",
    "        self.init_weights()\n",
    "\n",
    "    # decoder 가중치 초기화\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    # decoder에서 다음 값 예측 시 sequence의 다음 값을 모르게 하기 위해 마스킹 함수 정의\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    # 앞서 정의한 함수를 사용해 Transformer Encoder의 순전파 진행\n",
    "    def forward(self,src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask  # mask를 씌워 Mult-head Attn 수행\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return output\n",
    "\n",
    "\n",
    "class AttnDecoder(nn.Module):\n",
    "    # Transformer Decoder 구조 정의\n",
    "    def __init__(self, code_hidden_size, hidden_size, time_step):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.code_hidden_size = code_hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.T = time_step\n",
    "\n",
    "        # Model, Layer, Activation Fuction 정의\n",
    "        self.attn1 = nn.Linear(in_features=2 * hidden_size, out_features=code_hidden_size)\n",
    "        self.attn2 = nn.Linear(in_features=code_hidden_size, out_features=code_hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.attn3 = nn.Linear(in_features=code_hidden_size, out_features=1)\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=self.hidden_size,num_layers=1)\n",
    "        self.tilde = nn.Linear(in_features=self.code_hidden_size + 1, out_features=1)\n",
    "        self.fc1 = nn.Linear(in_features=code_hidden_size + hidden_size, out_features=hidden_size)\n",
    "        self.fc2 = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "\n",
    "    # 인풋 사이즈의 0값을 갖는 초기 텐서 생성\n",
    "    def init_variable(self, *args):\n",
    "        zero_tensor = torch.zeros(args)\n",
    "        return Variable(zero_tensor)\n",
    "\n",
    "    # hidden layer embedding 순서값을 바꾸어 줌\n",
    "    def embedding_hidden(self, x):\n",
    "        return x.permute(1, 0, 2)\n",
    "\n",
    "    # 앞서 정의한 함수를 사용해 Transformer Decoder의 순전파 진행\n",
    "    def forward(self, h, y_seq):\n",
    "        h_ = h.transpose(0,1) \n",
    "        batch_size = h.size(0)\n",
    "        d = self.init_variable(1, batch_size, self.hidden_size)\n",
    "        s = self.init_variable(1, batch_size, self.hidden_size)\n",
    "        h_0 = self.init_variable(1,batch_size, self.hidden_size)\n",
    "        h_ = torch.cat((h_0,h_),dim=0)\n",
    "\n",
    "        for t in range(self.T):\n",
    "            x = torch.cat((d,h_[t,:,:].unsqueeze(0)), 2)\n",
    "            h1 = self.attn1(x)\n",
    "            _, states = self.lstm(y_seq[:,t].unsqueeze(0).unsqueeze(2), (h1, s))\n",
    "            d = states[0]\n",
    "            s = states[1]\n",
    "        y_res = self.fc2(self.fc1(torch.cat((d.squeeze(0), h_[-1,:,:]), dim=1)))\n",
    "        return y_res\n",
    "\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, file_path, T=time_step, train_flag=True):\n",
    "        # KOSPI 데이터 불러오기\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as fp:\n",
    "            data_pd = pd.read_csv(fp)\n",
    "        self.train_flag = train_flag  # 학습용 데이터를 True로 설정\n",
    "        self.data_train_ratio = 0.9  # 90%를 학습용 데이터로 사용\n",
    "        self.T = T\n",
    "        \n",
    "        # 학습용 데이터인 경우\n",
    "        if train_flag:\n",
    "            self.data_len = int(self.data_train_ratio * len(data_pd))\n",
    "            data_all = np.array(data_pd['close'])  # KOSPI 종가 데이터 셋 활용\n",
    "            data_all = (data_all - np.mean(data_all)) / np.std(data_all)  # 데이터 셋 표준화\n",
    "            self.data = data_all[ : self.data_len]\n",
    "        \n",
    "        # 평가용 데이터인 경우\n",
    "        else:\n",
    "            self.data_len = int((1-self.data_train_ratio) * len(data_pd))\n",
    "            data_all = np.array(data_pd['close'])\n",
    "            data_all = (data_all-np.mean(data_all))/np.std(data_all)\n",
    "            self.data = data_all[-self.data_len:]\n",
    "        print(\"data len:{}\".format(self.data_len))  # 학습 시 학습/평가 데이터 개수 출력\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len-self.T\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx:idx+self.T], self.data[idx+self.T]\n",
    "\n",
    "\n",
    "def l2_loss(pred, label):\n",
    "    loss = torch.nn.functional.mse_loss(pred, label, size_average=True)  # MSE Loss 사용\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_once(encoder, decoder, dataloader, encoder_optim, decoder_optim):\n",
    "    # encoder, decoder 각각의 모델을 학습 단계로 설정\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    loader = tqdm.tqdm(dataloader)\n",
    "\n",
    "    loss_epoch = 0\n",
    "\n",
    "    for idx, (data, label) in enumerate(loader):\n",
    "        data_x = data.unsqueeze(2)\n",
    "        data_tran = data_x.transpose(0,1)\n",
    "        data_x, label ,data_y= data_tran.float(), label.float() ,data.float()\n",
    "        code_hidden = encoder(data_x)  # batch_size(64) 별로 전체 데이터를 나누어서 encoder 학습 진행\n",
    "        code_hidden = code_hidden.transpose(0,1)\n",
    "        output = decoder(code_hidden, data_y)  # batch_size(64) 별로 전체 데이터를 나누어서 decoder 학습 진행\n",
    "\n",
    "        encoder_optim.zero_grad()  # epoch 한 번의 학습이 완료되어지면 gradient를 항상 0으로 초기화\n",
    "        decoder_optim.zero_grad()\n",
    "        loss = l2_loss(output.squeeze(1), label)  # 손실 함수는 MSE Loss로 설정\n",
    "        loss.backward()  # 역전파 진행\n",
    "        \n",
    "        encoder_optim.step()  # 역전파 단계에서 수집된 변화도로 매개변수 조정\n",
    "        decoder_optim.step()\n",
    "        loss_epoch += loss.detach().item()  # 각 epoch 별 loss 출력\n",
    "    loss_epoch /= len(loader)\n",
    "    return loss_epoch\n",
    "\n",
    "\n",
    "def eval_once(encoder,decoder, dataloader):\n",
    "    # encoder, decoder 각각의 모델을 평가 단계로 설정\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    loader = tqdm.tqdm(dataloader)\n",
    "\n",
    "    loss_epoch = 0\n",
    "\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    for idx, (data, label) in enumerate(loader):\n",
    "        # data: batch, time x 1\n",
    "        data_x = data.unsqueeze(2)\n",
    "        data_x, label ,data_y= data_x.float(), label.float(), data.float()\n",
    "        code_hidden = encoder(data_x)  # encoder를 거쳐 code_hidden 출력\n",
    "        output = decoder(code_hidden, data_y).squeeze(1)  # decoder를 거쳐 output 출력\n",
    "        loss = l2_loss(output, label)  # 손실함수는 MSE Loss로 설정\n",
    "        loss_epoch += loss.detach().item()  # 각 epoch 별 loss 출력\n",
    "        preds += (output.detach().tolist())  # 예측값 preds를 리스트에 추가\n",
    "        labels += (label.detach().tolist())  # 정답값 label을 리스트에 추가\n",
    "    \n",
    "    preds = torch.Tensor(preds)  # 각 예측값과 정답값을 Tensor 형태로 변환\n",
    "    labels = torch.Tensor(labels)\n",
    "    \n",
    "    # 각 예측값과 정답값 계산\n",
    "    pred1 = preds[:-1]\n",
    "    pred2 = preds[1:]\n",
    "    pred_ = preds[1:]>preds[:-1]\n",
    "    label1 = labels[:-1]\n",
    "    label2 = labels[1:]\n",
    "    label_ = labels[1:]>labels[:-1]\n",
    "    \n",
    "    accuracy = (label_ == pred_).sum() / len(pred1)  # 앞서 정의한 예측값과 정답값을 기준으로 accuracy 계산\n",
    "    loss_epoch /= len(loader)  # 앞서 정의한 예측값과 정답값을 기준으로 loss 값 계산\n",
    "    return loss_epoch, accuracy\n",
    "\n",
    "\n",
    "def eval_plot(encoder, decoder, dataloader):\n",
    "    dataloader.shuffle = False  # 평가 단계이므로 shuffle=False로 설정\n",
    "    preds = []\n",
    "    labels = []\n",
    "    # encoder, decoder 각각의 모델을 평가 단계로 설정\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    loader = tqdm.tqdm(dataloader)\n",
    "\n",
    "    for idx, (data, label) in enumerate(loader):\n",
    "        data_x = data.unsqueeze(2)\n",
    "        data_x, label, data_y = data_x.float(), label.float(), data.float()\n",
    "        code_hidden = encoder(data_x)  # encoder를 거쳐 core_hidden 출력\n",
    "        output = decoder(code_hidden, data_y)  # decoder를 거쳐 output 출력\n",
    "        preds += (output.detach().tolist())  # 예측값 preds를 리스트에 추가\n",
    "        labels += (label.detach().tolist())  # 정답값 label을 리스트에 추가\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    data_x = list(range(len(preds)))\n",
    "    \n",
    "    ax.plot(data_x, preds, label='predict', color='red')  # 빨간색으로 예측값 lineplot 생성\n",
    "    ax.plot(data_x, labels,label='ground truth', color='blue')  # 파란색으로 정답값 lineplot 생성\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def main():\n",
    "    # train, val 데이터 셋 불러오기\n",
    "    dataset_train = StockDataset(file_path=args.data_path)\n",
    "    dataset_val = StockDataset(file_path=args.data_path, train_flag=False\n",
    "    train_loader = DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(dataset_val, batch_size=64, shuffle=False)\n",
    "\n",
    "    encoder = TransAm()  # encoder는 앞서 정의한 TransAM 함수 사용\n",
    "    decoder = AttnDecoder(code_hidden_size=64, hidden_size=64, time_step=time_step)  # decoder는 앞서 정의한 AttnDecoder 함수 사용\n",
    "    \n",
    "    # 각 encoder, decoder의 optimizer는 Adam으로 설정\n",
    "    encoder_optim = torch.optim.Adam(encoder.parameters(), lr=0.001)\n",
    "    decoder_optim = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
    "\n",
    "    total_epoch = 101  # 100 epoch까지 checkpoints 및 결과 plot 생성\n",
    "\n",
    "    for epoch_idx in range(total_epoch):\n",
    "        train_loss = train_once(encoder, decoder, train_loader, encoder_optim, decoder_optim)\n",
    "        print(\"stage: train, epoch:{:5d}, loss:{}\".format(epoch_idx, train_loss))\n",
    "        \n",
    "        if epoch_idx % 10 == 0:  # 10 epoch마다 평가용 데이터로 검증\n",
    "            eval_loss, accuracy = eval_once(encoder, decoder, val_loader)  # 평가를 진행해 eval_loss와 accuracy 계산\n",
    "            \n",
    "            print(\"##### stage: test, epoch:{:5d}, loss:{}, accuracy:{}\".format(epoch_idx, eval_loss, accuracy))  # 10번의 step마다 loss 출력\n",
    "            eval_plot(encoder,decoder, val_loader)\n",
    "            torch.save(encoder.state_dict(), \"{}/checkpoint_{:0>3}.ckpt\".format(checkpointdir, epoch_idx))  # 모델의 학습 가중치를 checkpoints로 저장\n",
    "            plt.savefig(\"{}/KOSPI_TRANSFORMER_epoch_{}.png\".format(plotdir, epoch_idx))  # 각 figure를 png 형태로 저장\n",
    "ention_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
